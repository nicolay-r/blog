---
layout: post
title: "\"X raise sanctions against Y\" - Finetune Sentiment Relation Extraction Model with AREkit and DeepPavlov [part 1/2]"
description: "\"X raise sanctions against Y\" - Finetune Sentiment Relation Extraction Model with AREkit and DeepPavlov [part 1/2]"
category: POST
tags: [Sentiment Analysis, Relation Extraction, DeepPavlov, Finetunning, Language Models, BERT]
comments: true
---

![alt text]({{site.url}}/img/arekit_deepPavlov-finetune.png)

**Sentiment attitude extraction** -- is a sentiment analysis subtask, in which attitude corresponds 
to the text position conveyed by Subject towards other Object mentioned in text such as: 
entities, events, etc. 

In this post we focused on the relation extraction between mentioned named entities in text, based on
**sampled contexts** generated by 
[AREkit](https://github.com/nicolay-r/AREkit)
and train [BERT classification model](https://arxiv.org/pdf/1810.04805.pdf) implemented by 
[DeepPavlov](https://deeppavlov.ai/) framework. 

<!--more-->

For a greater details related to samples preparation, for a particular text, 
please proceed with the following post on 
[Process Mass-Media relations for Language Models with AREkit](https://nicolay-r.github.io/blog/articles/2022-05/process-mass-media-relations-with-arekit)
or just proceed by downloading already prepared contexts (see "data preparation" details).

The code presented in a snippets below, could be manually executed within the following examples:
> [A COMPLETE EXAMPLE (BERT training data preparation)](https://github.com/nicolay-r/ARElight/blob/main/examples/serialize_rusentrel_for_bert.py)
>
> [A COMPLETE EXAMPLE (Training)](https://github.com/nicolay-r/ARElight/blob/main/examples/train_bert.py)

Let's get started with the required **data peraration** which requires the follwing steps:
1. Download RuSentRel prepared samples for BERT model training. 
[[download]](https://www.dropbox.com/s/iltg28qth6qjuhv/sample-train-0.tsv.gz?dl=1)
2. Download BERT-pretrained state on RuAttitudes collection 
[[download]](https://www.dropbox.com/s/g73osmwyrqtr2at/ra-20-srubert-large-neut-nli-pretrained-3l-finetuned.tar.gz?dl=1)

The input data represents and archive of the CSV formatter list of input contexts:
```python
# Declaring input-related parameters.
input_data = "sample-train-0.tsv.gz"
```

The initial step model is to provide references onto files of the BERT state:

```python
# Declaring BERT state related parameters. 
model_dir = "./ra-20-srubert-large-neut-nli-pretrained-3l"
bert_config_file = join(model_dir, "bert_config.json")
bert_ckpt_file = join(model_dir, "model.ckpt-30238")
vocab_file = join(model_dir, "vocab.txt")
do_lowercase = True

# Training parameters.
epochs_count = 4
batch_size = 2 
```

In order to perform samples classification, we adopt BERT classifier (`bert_classifier`) from [DeepPavlov (0.11.0)](https://deeppavlov.ai/) library.
According to the personal experience, **It is was important** to mention here a load path parameter: `load_path=model_checkpoint_path`.
Since only the latter allows to perform a complete model loading, including the classification layer ontop of the BERT backbone.

```python
# Model classifier.
model = bert_classifier.BertClassifierModel(
    bert_config_file=bert_config_file,
    load_path=bert_ckpt_file,
    keep_prob=0.1,
    n_classes=3,
    save_path="out",
    learning_rate=2e-5)

# Setup processor.
bert_proc = BertPreprocessor(vocab_file=vocab_file, 
                             do_lower_case=do_lowercase, 
                             max_seq_length=128)
```

AREkit-0.22.0 provides a `BaseRowsStorage` with API for samples iteration, implemented over `pandas` library.

The most **important notion here** was to perform shuffled data iteration. 
Most contexts are 0-labeled, it results in conspiracy during model training.
Among all the parameters, we keep `text_a`, `text_b` and `label`:

```python
def iter_batches(s, batch_size):
    assert(isinstance(s, BaseRowsStorage))

    data = {"text_a": [], "text_b": [], "label": []}

    # NOTE: it is important to iter shuffled data!
    for row_ind, row in s.iter_shuffled():
        data["text_a"].append(row['text_a'])
        data["text_b"].append(row['text_b'])
        data["label"].append(row['label'])

    for i in range(0, len(data["text_a"]), batch_size):

        texts_a = data["text_a"][i:i + batch_size]
        texts_b = data["text_b"][i:i + batch_size]
        labels = data["label"][i:i + batch_size]

        batch_features = bert_proc(texts_a=texts_a, texts_b=texts_b)

        yield batch_features, labels
```

We track the BERT sentiment classification model fine-tunning as follows:

```python
samples = BaseRowsStorage.from_tsv(input_data)
for e in range(epochs_count):

    it = iter_batches(samples, batch_size)
    batches = len(samples.DataFrame) / batch_size

    total_loss = 0
    pbar = tqdm(it, total=batches, desc="Epoch: {}".format(e), unit='batches')
    for batch_index, payload in enumerate(pbar):
        features, y = payload
        d = model.train_on_batch(features=features, y=y)
        total_loss += d["loss"]
        pbar.set_postfix({
            "avg-loss": total_loss/(batch_index+1)
        })
```

At last we keep the result model state as follows:
```pytnon
model.save()
```

In the next post we cover the pre-trained model application for unlabeled Mass-Media texts 

### Summary
The most important aspects deal with once experiment with BERT 
application for sentiment classification task were as follows:
1. Consider the classification layer hidden state loading as well 
(`load_path=model_checkpoint_path` for DeepPavlov library)
2. Guarantee shuffled and class-balanced input data.
