---
layout: post
title: "\"X raise sanctions against Y\" - Finetune Sentiment Relation Extraction Model with AREkit and DeepPavlov [part 1/2]"
description: "\"X raise sanctions against Y\" - Finetune Sentiment Relation Extraction Model with AREkit and DeepPavlov [part 1/2]"
category: POST
tags: [Sentiment Analysis, Relation Extraction, DeepPavlov, Finetunning, Language Models, BERT]
comments: true
---

![alt text]({{site.url}}/img/arekit_deepPavlov-finetune.png)

**Sentiment attitude extraction** [[6]](#references) -- is a sentiment analysis subtask, in which attitude corresponds 
to the text position conveyed by Subject towards other Object mentioned in text such as: 
entities, events, etc. 

In this post we focused on the relation extraction between mentioned named entities in text, based on
**sampled contexts** generated by 
[AREkit](https://github.com/nicolay-r/AREkit)
and train [BERT classification model](https://arxiv.org/pdf/1810.04805.pdf) implemented by 
[DeepPavlov](https://deeppavlov.ai/) framework. 

<!--more-->

For a greater details related to samples preparation, for a particular text, 
please proceed with the following post on 
[Process Mass-Media relations for Language Models with AREkit](https://nicolay-r.github.io/blog/articles/2022-05/process-mass-media-relations-with-arekit)
or just proceed by downloading already prepared contexts (see **data preparation** details)

Figure below illustrates an example of the CSV file utilized for finetunning:

![alt text](https://github.com/nicolay-r/ARElight/blob/main/docs/samples-bert.png/?raw=true)

The code presented in a snippets below, could be manually executed within the following examples:
> [A COMPLETE EXAMPLE (BERT training data preparation)](https://github.com/nicolay-r/ARElight/tree/0.22.0/examples/serialize_rusentrel_for_bert.py)
>
> [A COMPLETE EXAMPLE (Training)](https://github.com/nicolay-r/ARElight/tree/0.22.0/examples/train_bert.py)

Let's get started with the required **data peraration** stage.
For Sentiment Relation Extraction we consider and focusing on mass-media texts written in Russian.

To prepare the related text samples, we consider the contents of [RuSentRel](https://github.com/nicolay-r/RuSentRel) collection which represents 
analytical articles from `insomi.ru` portal, written in Russian.

As for the BERT initial state, we consider BERT-base-mult (multilingual BASE model) which 
has been finetunned with texts and news titles from [RuAttitudes-2.0-Large](https://github.com/nicolay-r/RuAttitudes) collection.
The latter represents a distant-supervision approach ([knowledge-based](https://github.com/nicolay-r/RuSentiFrames), see paper [[5]](#references)) 
towards mass-media collection of news,
and includes `~135K` news with annotated attitudes between mentioned named entities.
For a greater details please proceed with the related [[3]](#references) or [paper](https://aclanthology.org/R19-1118/).

To summarise, we consider the download the following resources:
1. [RuSentRel](https://github.com/nicolay-r/RuSentRel) prepared samples for BERT model training
[[download]](https://www.dropbox.com/s/iltg28qth6qjuhv/sample-train-0.tsv.gz?dl=1)
2. BERT-pretrained state 
[[download]](https://www.dropbox.com/s/g73osmwyrqtr2at/ra-20-srubert-large-neut-nli-pretrained-3l-finetuned.tar.gz?dl=1)

The input data represents and archive of the CSV formatter list of input contexts:
```python
# Declaring input-related parameters.
input_data = "sample-train-0.tsv.gz"
```

The initial step model is to provide references onto files of the BERT state:

```python
# Declaring BERT state related parameters. 
model_dir = "./ra-20-srubert-large-neut-nli-pretrained-3l"
bert_config_file = join(model_dir, "bert_config.json")
bert_ckpt_file = join(model_dir, "model.ckpt-30238")
vocab_file = join(model_dir, "vocab.txt")
do_lowercase = True

# Training parameters.
epochs_count = 4
batch_size = 2 
```

In order to perform samples classification, we adopt BERT classifier (`bert_classifier`) from [DeepPavlov (0.11.0)](https://deeppavlov.ai/) library.

According to the personal experience, **It is was important** to mention here a load path parameter: `load_path=model_checkpoint_path`.
Since only the latter allows to perform a complete model loading, including the classification layer ontop of the BERT backbone.

```python
# Model classifier.
model = bert_classifier.BertClassifierModel(
    bert_config_file=bert_config_file,
    load_path=bert_ckpt_file, # IMPORTANT: intialize classification layer!
    keep_prob=0.1,
    n_classes=3,
    save_path="out",
    learning_rate=2e-5)

# Setup processor.
bert_proc = BertPreprocessor(vocab_file=vocab_file, 
                             do_lower_case=do_lowercase, 
                             max_seq_length=128)
```

AREkit-0.22.0 provides a `BaseRowsStorage` with API for samples iteration, implemented over `pandas` library.

The most **important notion here** was to perform shuffled data iteration. 
Most contexts are 0-labeled, it results in conspiracy during model training.
Among all the parameters, we keep `text_a`, `text_b` and `label`:

```python
def iter_batches(s, batch_size):
    assert(isinstance(s, BaseRowsStorage))

    data = {"text_a": [], "text_b": [], "label": []}

    # NOTE: it is important to iter shuffled data!
    for row_ind, row in s.iter_shuffled():      # IMPORTANT: shuffle data!
        data["text_a"].append(row['text_a'])
        data["text_b"].append(row['text_b'])
        data["label"].append(row['label'])

    for i in range(0, len(data["text_a"]), batch_size):

        texts_a = data["text_a"][i:i + batch_size]
        texts_b = data["text_b"][i:i + batch_size]
        labels = data["label"][i:i + batch_size]

        batch_features = bert_proc(texts_a=texts_a, texts_b=texts_b)

        yield batch_features, labels
```

We track the BERT sentiment classification model fine-tunning as follows:

```python
samples = BaseRowsStorage.from_tsv(input_data)
for e in range(epochs_count):

    it = iter_batches(samples, batch_size)
    batches = len(samples.DataFrame) / batch_size

    total_loss = 0
    pbar = tqdm(it, total=batches, desc="Epoch: {}".format(e), unit='batches')
    for batch_index, payload in enumerate(pbar):
        features, y = payload
        d = model.train_on_batch(features=features, y=y)
        total_loss += d["loss"]
        pbar.set_postfix({
            "avg-loss": total_loss/(batch_index+1)
        })
```

At last we keep the result model state as follows:
```pytnon
model.save()
```

Let's take a look on how it affects on the result. 
In order to analyse attention weights we adopt approach [[1]](#references).
Considering a `HEAD#2` of the BERT transformer for layers (from left to right): `2`, `4`, `8`, `11`.
In addition, you may also checkout for a greater details the following paper [[2]](#references).

For the **pretrained state**:

![alt text]({{site.url}}/img/example_bert_2-4-8-11-head2-m2-pretrained.png)

1. This state has been fine-tuned by contents of the [RuAttitudes-2.0-Large](https://github.com/nicolay-r/RuAttitudes) collection 
 which yields of a large amount of news titles with annotated subject-object pairs and
 mentioned frames in between them; 
2. Smoothed attention distribution onto the top layers (11) is a specifics of the fine-tunning organization
adopted for `SentRuBERT` [[4]](#references).


After `4` epochs of the **fine-tunned** state:

![alt text]({{site.url}}/img/example_bert_2-4-8-11-head2-m2-finetuned.png)

1. It is possible to investigate the greater attention onto `#0` and `#S` objects in sample
 (on top layers)
2. Attention become greater for frames of the [RuSentiFrames](https://github.com/nicolay-r/RuSentiFrames) collection; 
the latter utlized in RuAttitudes development as a knowledge-base.

In the next post we cover the pre-trained model application for unlabeled Mass-Media texts 
using BRAT toolset as a front-end.

### Summary
The most important aspects deal with once experiment with BERT 
application for sentiment classification task were as follows:
1. Consider the classification layer hidden state loading as well 
(`load_path=model_checkpoint_path` for DeepPavlov library)
2. Guarantee shuffled and class-balanced input data.


### References

1. [What Does BERT Look at? An Analysis of BERTâ€™s Attention](https://aclanthology.org/W19-4828.pdf) + [[github]](https://github.com/clarkkev/attention-analysis)
2. [Language Models Application in Sentiment
Attitude Extraction Task](https://nicolay-r.github.io/website/data/rusnachenko2021language.pdf)
3. [Distant Supervision for Sentiment Attitude Extraction](https://aclanthology.org/R19-1118.pdf)
4. [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
5. [Sentiment Frames for Attitude Extraction in Russian](https://arxiv.org/pdf/2006.10973.pdf)
6. [Extracting Sentiment Attitudes from Analytical Texts](https://arxiv.org/pdf/1808.08932.pdf)